import asyncio
import copy
import json
import logging
import os
import urllib.request

import requests
from dotenv import load_dotenv
from flask import Flask, Response, jsonify, request, send_from_directory

load_dotenv()

app = Flask(__name__, static_folder="static")


# Static Files
@app.route("/")
def index():
    return app.send_static_file("index.html")


@app.route("/favicon.ico")
def favicon():
    return app.send_static_file("favicon.ico")


@app.route("/assets/<path:path>")
def assets(path):
    return send_from_directory("static/assets", path)


# Debug settings
DEBUG = os.environ.get("DEBUG", "false")
DEBUG_LOGGING = DEBUG.lower() == "true"
if DEBUG_LOGGING:
    logging.basicConfig(level=logging.DEBUG)

# On Your Data Settings
DATASOURCE_TYPE = os.environ.get("DATASOURCE_TYPE", "AzureCognitiveSearch")
SEARCH_TOP_K = os.environ.get("SEARCH_TOP_K", 5)
SEARCH_STRICTNESS = os.environ.get("SEARCH_STRICTNESS", 3)
SEARCH_ENABLE_IN_DOMAIN = os.environ.get("SEARCH_ENABLE_IN_DOMAIN", "true")

# ACS Integration Settings
AZURE_SEARCH_SERVICE = os.environ.get("AZURE_SEARCH_SERVICE")
AZURE_SEARCH_KEY = os.environ.get("AZURE_SEARCH_KEY")
AZURE_SEARCH_USE_SEMANTIC_SEARCH = os.environ.get(
    "AZURE_SEARCH_USE_SEMANTIC_SEARCH", "false"
)
AZURE_SEARCH_SEMANTIC_SEARCH_CONFIG = os.environ.get(
    "AZURE_SEARCH_SEMANTIC_SEARCH_CONFIG", "default"
)
AZURE_SEARCH_TOP_K = os.environ.get("AZURE_SEARCH_TOP_K", SEARCH_TOP_K)
AZURE_SEARCH_ENABLE_IN_DOMAIN = os.environ.get(
    "AZURE_SEARCH_ENABLE_IN_DOMAIN", SEARCH_ENABLE_IN_DOMAIN
)
AZURE_SEARCH_CONTENT_COLUMNS = os.environ.get("AZURE_SEARCH_CONTENT_COLUMNS")
AZURE_SEARCH_FILENAME_COLUMN = os.environ.get("AZURE_SEARCH_FILENAME_COLUMN")
AZURE_SEARCH_TITLE_COLUMN = os.environ.get("AZURE_SEARCH_TITLE_COLUMN")
AZURE_SEARCH_URL_COLUMN = os.environ.get("AZURE_SEARCH_URL_COLUMN")
AZURE_SEARCH_VECTOR_COLUMNS = os.environ.get("AZURE_SEARCH_VECTOR_COLUMNS")
AZURE_SEARCH_QUERY_TYPE = os.environ.get("AZURE_SEARCH_QUERY_TYPE")
AZURE_SEARCH_PERMITTED_GROUPS_COLUMN = os.environ.get(
    "AZURE_SEARCH_PERMITTED_GROUPS_COLUMN"
)
AZURE_SEARCH_STRICTNESS = os.environ.get("AZURE_SEARCH_STRICTNESS", SEARCH_STRICTNESS)
AZURE_SEARCH_INDEX_GRANTS = os.environ.get("AZURE_SEARCH_INDEX_GRANTS")
AZURE_SEARCH_INDEX_ARTICLES = os.environ.get("AZURE_SEARCH_INDEX_ARTICLES")

# AOAI Integration Settings
AZURE_OPENAI_RESOURCE = os.environ.get("AZURE_OPENAI_RESOURCE")
AZURE_OPENAI_MODEL = os.environ.get("AZURE_OPENAI_MODEL")
AZURE_OPENAI_ENDPOINT = os.environ.get("AZURE_OPENAI_ENDPOINT")
AZURE_OPENAI_KEY = os.environ.get("AZURE_OPENAI_KEY")
AZURE_OPENAI_TEMPERATURE = os.environ.get("AZURE_OPENAI_TEMPERATURE", 0)
AZURE_OPENAI_TOP_P = os.environ.get("AZURE_OPENAI_TOP_P", 1.0)
AZURE_OPENAI_MAX_TOKENS = os.environ.get("AZURE_OPENAI_MAX_TOKENS", 1000)
AZURE_OPENAI_STOP_SEQUENCE = os.environ.get("AZURE_OPENAI_STOP_SEQUENCE")
AZURE_OPENAI_SYSTEM_MESSAGE = os.environ.get(
    "AZURE_OPENAI_SYSTEM_MESSAGE",
    "You are an AI assistant that helps people find information.",
)
AZURE_OPENAI_PREVIEW_API_VERSION = os.environ.get(
    "AZURE_OPENAI_PREVIEW_API_VERSION", "2023-08-01-preview"
)
AZURE_OPENAI_STREAM = os.environ.get("AZURE_OPENAI_STREAM", "true")
AZURE_OPENAI_MODEL_NAME = os.environ.get(
    "AZURE_OPENAI_MODEL_NAME", "gpt-35-turbo-16k"
)  # Name of the model, e.g. 'gpt-35-turbo-16k' or 'gpt-4'
AZURE_OPENAI_EMBEDDING_ENDPOINT = os.environ.get("AZURE_OPENAI_EMBEDDING_ENDPOINT")
AZURE_OPENAI_EMBEDDING_KEY = os.environ.get("AZURE_OPENAI_EMBEDDING_KEY")
AZURE_OPENAI_EMBEDDING_NAME = os.environ.get("AZURE_OPENAI_EMBEDDING_NAME", "")
AZURE_OPENAI_API_TYPE = os.environ.get("AZURE_OPENAI_API_TYPE")
AISTUDIO_API_KEY = os.environ.get("AISTUDIO_API_KEY")
USE_AZURE_AI_STUDIO = os.environ.get("USE_AZURE_AI_STUDIO", "False")

SHOULD_STREAM = True if AZURE_OPENAI_STREAM.lower() == "true" else False

# Frontend Settings via Environment Variables
AUTH_ENABLED = os.environ.get("AUTH_ENABLED", "true").lower()
frontend_settings = {"auth_enabled": AUTH_ENABLED}

loop = asyncio.new_event_loop()
asyncio.set_event_loop(loop)


def is_chat_model():
    if (
        "gpt-4" in AZURE_OPENAI_MODEL_NAME.lower()
        or AZURE_OPENAI_MODEL_NAME.lower() in ["gpt-35-turbo-4k", "gpt-35-turbo-16k"]
    ):
        return True
    return False


def should_use_data():
    if AZURE_SEARCH_SERVICE and AZURE_SEARCH_KEY:
        if DEBUG_LOGGING:
            logging.debug("Using Azure Cognitive Search")
        return True

    return False


def format_as_ndjson(obj: dict) -> str:
    return json.dumps(obj, ensure_ascii=False) + "\n"


def parse_multi_columns(columns: str) -> list:
    if "|" in columns:
        return columns.split("|")
    else:
        return columns.split(",")


def fetchUserGroups(userToken, nextLink=None):
    # Recursively fetch group membership
    if nextLink:
        endpoint = nextLink
    else:
        endpoint = "https://graph.microsoft.com/v1.0/me/transitiveMemberOf?$select=id"

    headers = {"Authorization": "bearer " + userToken}
    try:
        r = requests.get(endpoint, headers=headers)
        if r.status_code != 200:
            if DEBUG_LOGGING:
                logging.error(f"Error fetching user groups: {r.status_code} {r.text}")
            return []

        r = r.json()
        if "@odata.nextLink" in r:
            nextLinkData = fetchUserGroups(userToken, r["@odata.nextLink"])
            r["value"].extend(nextLinkData)

        return r["value"]
    except Exception as e:
        logging.error(f"Exception in fetchUserGroups: {e}")
        return []


def generateFilterString(userToken):
    # Get list of groups user is a member of
    userGroups = fetchUserGroups(userToken)

    # Construct filter string
    if not userGroups:
        logging.debug("No user groups found")

    group_ids = ", ".join([obj["id"] for obj in userGroups])
    return f"{AZURE_SEARCH_PERMITTED_GROUPS_COLUMN}/any(g:search.in(g, '{group_ids}'))"


def prepare_body_headers_with_data(request):
    request_messages = request.json["messages"]

    body = {
        "messages": request_messages,
        "temperature": float(AZURE_OPENAI_TEMPERATURE),
        "max_tokens": int(AZURE_OPENAI_MAX_TOKENS),
        "top_p": float(AZURE_OPENAI_TOP_P),
        "stop": (
            AZURE_OPENAI_STOP_SEQUENCE.split("|")
            if AZURE_OPENAI_STOP_SEQUENCE
            else None
        ),
        "stream": SHOULD_STREAM,
        "dataSources": [],
    }

    if DATASOURCE_TYPE == "AzureCognitiveSearch":
        # Set query type
        query_type = "simple"
        if AZURE_SEARCH_QUERY_TYPE:
            query_type = AZURE_SEARCH_QUERY_TYPE
        elif (
            AZURE_SEARCH_USE_SEMANTIC_SEARCH.lower() == "true"
            and AZURE_SEARCH_SEMANTIC_SEARCH_CONFIG
        ):
            query_type = "semantic"

        # Set filter
        filter = None
        userToken = None
        if AZURE_SEARCH_PERMITTED_GROUPS_COLUMN:
            userToken = request.headers.get("X-MS-TOKEN-AAD-ACCESS-TOKEN", "")
            if DEBUG_LOGGING:
                logging.debug(
                    f"USER TOKEN is {'present' if userToken else 'not present'}"
                )

            filter = generateFilterString(userToken)
            if DEBUG_LOGGING:
                logging.debug(f"FILTER: {filter}")

        body["dataSources"].append(
            {
                "type": "AzureCognitiveSearch",
                "parameters": {
                    "endpoint": f"https://{AZURE_SEARCH_SERVICE}.search.windows.net",
                    "key": AZURE_SEARCH_KEY,
                    "indexName": (
                        AZURE_SEARCH_INDEX_GRANTS
                        if request.json.get("index_name").lower() == "grants"
                        else AZURE_SEARCH_INDEX_ARTICLES
                    ),
                    "fieldsMapping": {
                        "contentFields": (
                            parse_multi_columns(AZURE_SEARCH_CONTENT_COLUMNS)
                            if AZURE_SEARCH_CONTENT_COLUMNS
                            else []
                        ),
                        "titleField": (
                            AZURE_SEARCH_TITLE_COLUMN
                            if AZURE_SEARCH_TITLE_COLUMN
                            else None
                        ),
                        "urlField": (
                            AZURE_SEARCH_URL_COLUMN if AZURE_SEARCH_URL_COLUMN else None
                        ),
                        "filepathField": (
                            AZURE_SEARCH_FILENAME_COLUMN
                            if AZURE_SEARCH_FILENAME_COLUMN
                            else None
                        ),
                        "vectorFields": (
                            parse_multi_columns(AZURE_SEARCH_VECTOR_COLUMNS)
                            if AZURE_SEARCH_VECTOR_COLUMNS
                            else []
                        ),
                    },
                    "inScope": (
                        True
                        if AZURE_SEARCH_ENABLE_IN_DOMAIN.lower() == "true"
                        else False
                    ),
                    "topNDocuments": AZURE_SEARCH_TOP_K,
                    "queryType": query_type,
                    "semanticConfiguration": (
                        AZURE_SEARCH_SEMANTIC_SEARCH_CONFIG
                        if AZURE_SEARCH_SEMANTIC_SEARCH_CONFIG
                        else ""
                    ),
                    "roleInformation": AZURE_OPENAI_SYSTEM_MESSAGE,
                    "filter": filter,
                    "strictness": int(AZURE_SEARCH_STRICTNESS),
                },
            }
        )
    else:
        raise Exception(
            f"DATASOURCE_TYPE is not configured or unknown: {DATASOURCE_TYPE}"
        )

    if "vector" in query_type.lower():
        if AZURE_OPENAI_EMBEDDING_NAME:
            body["dataSources"][0]["parameters"][
                "embeddingDeploymentName"
            ] = AZURE_OPENAI_EMBEDDING_NAME
        else:
            body["dataSources"][0]["parameters"][
                "embeddingEndpoint"
            ] = AZURE_OPENAI_EMBEDDING_ENDPOINT
            body["dataSources"][0]["parameters"][
                "embeddingKey"
            ] = AZURE_OPENAI_EMBEDDING_KEY

    if DEBUG_LOGGING:
        body_clean = copy.deepcopy(body)
        if body_clean["dataSources"][0]["parameters"].get("key"):
            body_clean["dataSources"][0]["parameters"]["key"] = "*****"
        if body_clean["dataSources"][0]["parameters"].get("connectionString"):
            body_clean["dataSources"][0]["parameters"]["connectionString"] = "*****"
        if body_clean["dataSources"][0]["parameters"].get("embeddingKey"):
            body_clean["dataSources"][0]["parameters"]["embeddingKey"] = "*****"

        logging.debug(f"REQUEST BODY: {json.dumps(body_clean, indent=4)}")

    headers = {
        "Content-Type": "application/json",
        "api-key": AZURE_OPENAI_KEY,
        "x-ms-useragent": "GitHubSampleWebApp/PublicAPI/3.0.0",
    }

    return body, headers


def stream_with_data(body, headers, endpoint, history_metadata={}):
    if USE_AZURE_AI_STUDIO.lower() == "true":
        endpoint = os.environ.get("AI_STUDIO_CHAT_FLOW_ENDPOINT")
        api_key = os.environ.get("AI_STUDIO_CHAT_FLOW_API_KEY")
        headers = {
            "Content-Type": "application/json",
            "Authorization": ("Bearer " + api_key),
            "azureml-model-deployment": os.environ.get(
                "AI_STUDIO_CHAT_FLOW_DEPLOYMENT_NAME"
            ),
            "Accept": "text/event-stream",
        }

        s = requests.Session()
        try:
            with s.post(endpoint, json=body, headers=headers, stream=True) as r:
                for line in r.iter_lines(chunk_size=10):
                    try:
                        rawResponse = json.loads(line.lstrip(b"data:").decode("utf-8"))[
                            "answer"
                        ]
                        lineJson = json.loads(rawResponse)
                    except json.decoder.JSONDecodeError:
                        continue

                    if "error" in lineJson:
                        yield format_as_ndjson(lineJson)

                    yield format_as_ndjson(lineJson)
        except Exception as e:
            yield format_as_ndjson({"error" + str(e)})
    else:
        s = requests.Session()
        try:
            with s.post(endpoint, json=body, headers=headers, stream=True) as r:
                for line in r.iter_lines(chunk_size=10):
                    response = {
                        "id": "",
                        "model": "",
                        "created": 0,
                        "object": "",
                        "choices": [{"messages": []}],
                        "apim-request-id": "",
                        "history_metadata": history_metadata,
                    }

                    if line:
                        if AZURE_OPENAI_PREVIEW_API_VERSION == "2023-06-01-preview":
                            lineJson = json.loads(line.lstrip(b"data:").decode("utf-8"))
                        else:
                            try:
                                rawResponse = json.loads(
                                    line.lstrip(b"data:").decode("utf-8")
                                )
                                lineJson = formatApiResponseStreaming(rawResponse)
                            except json.decoder.JSONDecodeError:
                                continue

                        if "error" in lineJson:
                            error_code_value = lineJson.get("error", {}).get("code", "")
                            error_message = format_as_ndjson(lineJson)
                            inner_error_code_value = extract_value(
                                "code", error_message
                            )
                            inner_error_status_value = extract_value(
                                "status", error_message
                            )
                            if (
                                inner_error_code_value == "content_filter"
                                and inner_error_status_value == "400"
                            ):
                                response["choices"][0]["messages"].append(
                                    {
                                        "role": "assistant",
                                        "content": "I am sorry, I don’t have this information in the knowledge repository. Please ask another question.",
                                    }
                                )
                                yield format_as_ndjson(response)
                            elif (
                                error_code_value == "429"
                                or inner_error_code_value == "429"
                            ):
                                yield format_as_ndjson(
                                    {
                                        "error": "We're currently experiencing a high number of requests for the service you're trying to access. Please wait a moment and try again."
                                    }
                                )
                            else:
                                yield format_as_ndjson(
                                    {
                                        "error": "An error occurred. Please try again. If the problem persists, please contact the site administrator."
                                    }
                                )
                            continue
                        response["id"] = lineJson["id"]
                        response["model"] = lineJson["model"]
                        response["created"] = lineJson["created"]
                        response["object"] = lineJson["object"]
                        response["apim-request-id"] = r.headers.get("apim-request-id")

                        role = lineJson["choices"][0]["messages"][0]["delta"].get(
                            "role"
                        )

                        if role == "tool":
                            response["choices"][0]["messages"].append(
                                lineJson["choices"][0]["messages"][0]["delta"]
                            )
                            yield format_as_ndjson(response)
                        elif role == "assistant":
                            if response["apim-request-id"] and DEBUG_LOGGING:
                                logging.debug(
                                    f"RESPONSE apim-request-id: {response['apim-request-id']}"
                                )
                            response["choices"][0]["messages"].append(
                                {"role": "assistant", "content": ""}
                            )
                            yield format_as_ndjson(response)
                        else:
                            deltaText = lineJson["choices"][0]["messages"][0]["delta"][
                                "content"
                            ]
                            if deltaText != "[DONE]":
                                response["choices"][0]["messages"].append(
                                    {"role": "assistant", "content": deltaText}
                                )
                                yield format_as_ndjson(response)
        except Exception as e:
            yield format_as_ndjson({"error" + str(e)})


def formatApiResponseNoStreaming(rawResponse):
    if "error" in rawResponse:
        return {"error": rawResponse["error"]}
    response = {
        "id": rawResponse["id"],
        "model": rawResponse["model"],
        "created": rawResponse["created"],
        "object": rawResponse["object"],
        "choices": [{"messages": []}],
    }
    toolMessage = {
        "role": "tool",
        "content": rawResponse["choices"][0]["message"]["context"]["messages"][0][
            "content"
        ],
    }
    assistantMessage = {
        "role": "assistant",
        "content": rawResponse["choices"][0]["message"]["content"],
    }
    response["choices"][0]["messages"].append(toolMessage)
    response["choices"][0]["messages"].append(assistantMessage)

    return response


def formatApiResponseStreaming(rawResponse):
    if "error" in rawResponse:
        return {"error": rawResponse["error"]}
    response = {
        "id": rawResponse["id"],
        "model": rawResponse["model"],
        "created": rawResponse["created"],
        "object": rawResponse["object"],
        "choices": [{"messages": []}],
    }

    if rawResponse["choices"][0]["delta"].get("context"):
        messageObj = {
            "delta": {
                "role": "tool",
                "content": rawResponse["choices"][0]["delta"]["context"]["messages"][0][
                    "content"
                ],
            }
        }
        response["choices"][0]["messages"].append(messageObj)
    elif rawResponse["choices"][0]["delta"].get("role"):
        messageObj = {
            "delta": {
                "role": "assistant",
            }
        }
        response["choices"][0]["messages"].append(messageObj)
    else:
        if rawResponse["choices"][0]["end_turn"]:
            messageObj = {
                "delta": {
                    "content": "[DONE]",
                }
            }
            response["choices"][0]["messages"].append(messageObj)
        else:
            messageObj = {
                "delta": {
                    "content": rawResponse["choices"][0]["delta"]["content"],
                }
            }
            response["choices"][0]["messages"].append(messageObj)

    return response


def conversation_with_data(request_body):
    body, headers = prepare_body_headers_with_data(request)
    base_url = (
        AZURE_OPENAI_ENDPOINT
        if AZURE_OPENAI_ENDPOINT
        else f"https://{AZURE_OPENAI_RESOURCE}.openai.azure.com/"
    )
    endpoint = f"{base_url}openai/deployments/{AZURE_OPENAI_MODEL}/extensions/chat/completions?api-version={AZURE_OPENAI_PREVIEW_API_VERSION}"
    history_metadata = request_body.get("history_metadata", {})

    if USE_AZURE_AI_STUDIO.lower() == "true":
        body = request_body

    if not SHOULD_STREAM:
        r = requests.post(endpoint, headers=headers, json=body)
        status_code = r.status_code
        r = r.json()
        if AZURE_OPENAI_PREVIEW_API_VERSION == "2023-06-01-preview":
            r["history_metadata"] = history_metadata
            return Response(format_as_ndjson(r), status=status_code)
        else:
            result = formatApiResponseNoStreaming(r)
            result["history_metadata"] = history_metadata
            return Response(format_as_ndjson(result), status=status_code)
    else:

        return Response(
            stream_with_data(body, headers, endpoint, history_metadata),
            mimetype="text/event-stream",
        )


@app.route("/conversation", methods=["GET", "POST"])
def conversation():
    request_body = request.json
    return conversation_internal(request_body)


def conversation_internal(request_body):
    try:
        return conversation_with_data(request_body)
    except Exception as e:
        logging.exception("Exception in /conversation")
        return jsonify({"error": str(e)}), 500


@app.route("/frontend_settings", methods=["GET"])
def get_frontend_settings():
    try:
        return jsonify(frontend_settings), 200
    except Exception as e:
        logging.exception("Exception in /frontend_settings")
        return jsonify({"error": str(e)}), 500


def run_async(func):
    return loop.run_until_complete(func)


# Helper function to extract values safely
def extract_value(key, text, default="N/A"):
    try:
        return text.split(f"'{key}': ")[1].split(",")[0].strip("'")
    except IndexError:
        return default


@app.route("/draft_document/generate_section", methods=["POST"])
def draft_document_generate():
    request_body = request.json
    topic = request_body["grantTopic"]
    section = request_body["sectionTitle"]
    section_context = request_body["sectionContext"]
    if section_context != "":
        query = f"{section_context} "
    else:
        query = f"Create {section} section of research grant application for - {topic}."

    data = {
        "chat_history": [],
        "query": query,
    }
    body = str.encode(json.dumps(data))

    url = os.environ.get("AI_STUDIO_DRAFT_FLOW_ENDPOINT")
    api_key = os.environ.get("AI_STUDIO_DRAFT_FLOW_API_KEY")
    headers = {
        "Content-Type": "application/json",
        "Authorization": ("Bearer " + api_key),
        "azureml-model-deployment": os.environ.get(
            "AI_STUDIO_DRAFT_FLOW_DEPLOYMENT_NAME"
        ),
    }
    req = urllib.request.Request(url, body, headers)

    try:
        response = urllib.request.urlopen(req)
        result = response.read()
        return jsonify({"content": json.loads(result)["reply"]}), 200
    except urllib.error.HTTPError as error:
        # Read and parse the error response
        res = error.read()
        try:
            # Parse the error content as JSON
            error_json = json.loads(res)
        except json.JSONDecodeError:
            return "Failed to decode the error content."

        error_message = error_json["error"]["message"]
        # Extract specific parts of the error message
        code_value = extract_value("code", error_json["error"]["message"])
        status_value = extract_value("status", error_json["error"]["message"])

        if code_value == "content_filter" and status_value == "400":
            return (
                jsonify({"The request failed with status code: ": str(error_message)}),
                400,
            )
        else:
            return "The request failed with status code: " + str(error.code)


if __name__ == "__main__":
    app.run()
