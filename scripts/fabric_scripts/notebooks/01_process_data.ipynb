{"cells":[{"cell_type":"code","execution_count":null,"id":"ecbae670-d6fe-42f3-becc-385124598085","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["from pyspark.sql.functions import col, to_date,to_timestamp\n","from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, ArrayType, MapType, LongType, TimestampType, DateType\n","from datetime import datetime\n","import pandas as pd"]},{"cell_type":"code","execution_count":null,"id":"3b2dfa30-4bac-4823-b80d-1e0345c2918c","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["df = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\", \"true\").load(\"Files/data/clientdata/Retirement.csv\")\n","df = df.withColumn(\"ClientId\", col(\"ClientId\").cast(StringType()))\n","df = df.withColumn(\"StatusDate\", to_date(to_timestamp(df.StatusDate, 'M/d/yyyy')))\n","\n","# to adjust the dates to current date\n","df = df.toPandas()\n","df['StatusDate'] = pd.to_datetime(df['StatusDate'])\n","today = datetime.today()\n","days_difference = (today - max(df['StatusDate'])).days - 30\n","months_difference = int(days_difference/30)\n","df['StatusDate'] = df['StatusDate'] + pd.DateOffset(months=months_difference)\n","df['StatusDate'] = pd.to_datetime(df['StatusDate']).dt.date\n","df = spark.createDataFrame(df)\n","\n","df.write.format('delta').mode('overwrite').option(\"overwriteSchema\", \"true\").saveAsTable('Retirement')"]},{"cell_type":"code","execution_count":null,"id":"2662eecd-2435-43f9-9238-660c7c5a9317","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["df = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"Files/data/clientdata/Clients.csv\")\n","df = df.withColumn(\"ClientId\", col(\"ClientId\").cast(StringType()))\n","df = df.withColumn(\"Dependents\", col(\"Dependents\").cast(IntegerType()))\n","df.write.format('delta').mode('overwrite').option(\"overwriteSchema\", \"true\").saveAsTable('Clients')\n","# display(df)"]},{"cell_type":"code","execution_count":null,"id":"5c9a5873-fc84-4987-b552-417635446607","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["df = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"Files/data/clientdata/AccountDetails.csv\")\n","df = df.withColumn(\"ClientId\", col(\"ClientId\").cast(StringType()))\n","df = df.withColumn(\"AccountBalance\", col(\"AccountBalance\").cast(DoubleType()))\n","df.write.format('delta').mode('overwrite').option(\"overwriteSchema\", \"true\").saveAsTable('AccountDetails')\n","# display(df)"]},{"cell_type":"code","execution_count":null,"id":"21eeaaae-4ce0-4b70-b5c5-e467664e8308","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["df = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\", \"true\").load(\"Files/data/clientdata/Assets.csv\")\n","df = df.withColumn(\"ClientId\", col(\"ClientId\").cast(StringType()))\n","df = df.withColumn(\"Investment\", col(\"Investment\").cast(DoubleType()))\n","df = df.withColumn(\"AssetDate\", to_date(to_timestamp(df.AssetDate, 'M/d/yyyy')))\n","\n","df = df.toPandas()\n","#to adjust the dates to current date\n","df['AssetDate'] = pd.to_datetime(df['AssetDate'])\n","today = datetime.today()\n","days_difference = (today - max(df['AssetDate'])).days - 30\n","months_difference = int(days_difference/30)\n","\n","df['AssetDate'] = df['AssetDate'] + pd.DateOffset(months=months_difference)\n","\n","df['AssetDate'] = pd.to_datetime(df['AssetDate'], format='%m/%d/%Y') #   %Y-%m-%d')\n","df['ClientId'] = df['ClientId'].astype(int)\n","df['Investment'] = df['Investment'].astype(float)\n","df['ROI'] = df['ROI'].astype(float)\n","df['Revenue'] = df['Revenue'].astype(float)\n","\n","df = spark.createDataFrame(df)\n","df.write.format('delta').mode('overwrite').option(\"overwriteSchema\", \"true\").saveAsTable('Assets')\n"]},{"cell_type":"code","execution_count":null,"id":"1543f518-3954-4916-a530-d24462477837","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["df = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\", \"true\").load(\"Files/data/clientdata/ClientInvestmentPortfolio.csv\")\n","df = df.withColumn(\"ClientId\", col(\"ClientId\").cast(StringType()))\n","\n","df = df.withColumn(\"AssetDate\", to_date(to_timestamp(df.AssetDate, 'M/d/yyyy')))\n","df = df.withColumn(\"Investment\", col(\"Investment\").cast(DoubleType()))\n","df.write.format('delta').mode('overwrite').option(\"overwriteSchema\", \"true\").saveAsTable('ClientInvestmentPortfolio')"]},{"cell_type":"code","execution_count":null,"id":"39f78348-4a8f-4597-a6ca-391bb58839ff","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# from pyspark.sql.functions import col, to_date\n","# from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, MapType, LongType, TimestampType, DateType\n","# df = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\", \"true\").load(\"Files/data/clientdata/ClientMeetings.csv\")\n","# # df = df.withColumn(\"ClientId\", col(\"ClientId\").cast(StringType()))\n","\n","# df.write.format('delta').mode('overwrite').option(\"overwriteSchema\", \"true\").saveAsTable('ClientMeetings')"]},{"cell_type":"code","execution_count":null,"id":"652ca432-4fec-46a6-af42-e1bf9e67fa59","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["from pyspark.sql.functions import col, to_date\n","from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, MapType, LongType, TimestampType, DateType\n","df = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\", \"true\").load(\"Files/data/clientdata/InvestmentGoals.csv\")\n","df = df.withColumn(\"ClientId\", col(\"ClientId\").cast(StringType()))\n","df.write.format('delta').mode('overwrite').option(\"overwriteSchema\", \"true\").saveAsTable('InvestmentGoals')"]},{"cell_type":"code","execution_count":null,"id":"38da0bcc-e159-4ebd-b214-020916fbe89a","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["df = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\", \"true\").load(\"Files/data/clientdata/InvestmentGoalsDetails.csv\")\n","df = df.withColumn(\"ClientId\", col(\"ClientId\").cast(StringType()))\n","df.write.format('delta').mode('overwrite').option(\"overwriteSchema\", \"true\").saveAsTable('InvestmentGoalsDetails')"]}],"metadata":{"dependencies":{"lakehouse":{"default_lakehouse":"851083bf-f5d2-4d00-a965-0a098d89f6dc","default_lakehouse_name":"WealthAdvisor","default_lakehouse_workspace_id":"1011789c-c9cb-4dd1-bb66-7a81f5f567e5","known_lakehouses":[{"id":"851083bf-f5d2-4d00-a965-0a098d89f6dc"}]}},"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","language":"Python","name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default"},"synapse_widget":{"state":{},"version":"0.1"},"widgets":{}},"nbformat":4,"nbformat_minor":5}
