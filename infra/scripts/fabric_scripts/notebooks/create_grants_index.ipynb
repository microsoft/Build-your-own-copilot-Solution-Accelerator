{"cells":[{"cell_type":"code","execution_count":null,"id":"33dc6091-e0c6-403b-bb1e-935336803c57","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# %pip install azure-ai-textanalytics\n","# %pip install azure-search-documents\n","# %pip install openai --upgrade\n","\n","## %pip install langchain\n","# %pip install pypdf\n","# %pip install tiktoken"]},{"cell_type":"code","execution_count":null,"id":"02278b5f-06e2-4f1d-a11a-7030c696b06d","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["#Get Azure Key Vault Client\n","key_vault_name = 'kv_to-be-replaced'\n","\n","index_name = \"grantsindex\"\n","drafts_index_name = 'draftsindex'\n","file_system_client = \"data\"\n","directory = 'demodata/nih_grants' \n","csv_file_name = 'nih_grants.csv'\n","\n","num_pages = 10"]},{"cell_type":"code","execution_count":null,"id":"c6fe9e75-4932-47e5-a40a-5be9ecf4cd6c","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["from trident_token_library_wrapper import PyTridentTokenLibrary as tl\n","\n","def get_secrets_from_kv(kv_name, secret_name):\n","\n","    access_token = mssparkutils.credentials.getToken(\"keyvault\")\n","    kv_endpoint = f'https://{kv_name}.vault.azure.net/'\n","    return(tl.get_secret_with_token(kv_endpoint,secret_name,access_token))\n"]},{"cell_type":"code","execution_count":null,"id":"c75f837e-c9ef-4ff3-8183-8e61dbe1cf76","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Import required libraries  \n","import os  \n","import json  \n","import openai\n","\n","import os  \n","from azure.core.credentials import AzureKeyCredential  \n","from azure.ai.textanalytics import TextAnalyticsClient  \n","\n","from azure.core.credentials import AzureKeyCredential  \n","from azure.search.documents import SearchClient, SearchIndexingBufferedSender  \n","from azure.search.documents.indexes import SearchIndexClient  \n","from azure.search.documents.models import (\n","    QueryAnswerType,\n","    QueryCaptionType,\n","    QueryCaptionResult,\n","    QueryAnswerResult,\n","    SemanticErrorMode,\n","    SemanticErrorReason,\n","    SemanticSearchResultsType,\n","    QueryType,\n","    VectorizedQuery,\n","    VectorQuery,\n","    VectorFilterMode,    \n",")\n","from azure.search.documents.indexes.models import (  \n","    ExhaustiveKnnAlgorithmConfiguration,\n","    ExhaustiveKnnParameters,\n","    SearchIndex,  \n","    SearchField,  \n","    SearchFieldDataType,  \n","    SimpleField,  \n","    SearchableField,  \n","    SearchIndex,  \n","    SemanticConfiguration,  \n","    SemanticPrioritizedFields,\n","    SemanticField,  \n","    SearchField,  \n","    SemanticSearch,\n","    VectorSearch,  \n","    HnswAlgorithmConfiguration,\n","    HnswParameters,  \n","    VectorSearch,\n","    VectorSearchAlgorithmConfiguration,\n","    VectorSearchAlgorithmKind,\n","    VectorSearchProfile,\n","    SearchIndex,\n","    SearchField,\n","    SearchFieldDataType,\n","    SimpleField,\n","    SearchableField,\n","    VectorSearch,\n","    ExhaustiveKnnParameters,\n","    SearchIndex,  \n","    SearchField,  \n","    SearchFieldDataType,  \n","    SimpleField,  \n","    SearchableField,  \n","    SearchIndex,  \n","    SemanticConfiguration,  \n","    SemanticField,  \n","    SearchField,  \n","    VectorSearch,  \n","    HnswParameters,  \n","    VectorSearch,\n","    VectorSearchAlgorithmKind,\n","    VectorSearchAlgorithmMetric,\n","    VectorSearchProfile,\n",")  \n","search_endpoint =  get_secrets_from_kv(key_vault_name,\"AZURE-SEARCH-SERVICE-ENDPOINT\")\n","search_key =  get_secrets_from_kv(key_vault_name,\"AZURE-SEARCH-ADMIN-KEY\")\n","\n","openai.api_type = get_secrets_from_kv(key_vault_name,\"OPENAI-API-TYPE\")\n","openai.api_key  = get_secrets_from_kv(key_vault_name,\"AZURE-OPENAI-API-KEY\")\n","openai.api_base = get_secrets_from_kv(key_vault_name,\"AZURE-OPENAI-ENDPOINT\")\n","openai.api_version = get_secrets_from_kv(key_vault_name,\"AZURE-OPENAI-API-VERSION\")\n","\n","openai_api_type = get_secrets_from_kv(key_vault_name,\"OPENAI-API-TYPE\")\n","openai_api_key  = get_secrets_from_kv(key_vault_name,\"AZURE-OPENAI-API-KEY\")\n","openai_api_base = get_secrets_from_kv(key_vault_name,\"AZURE-OPENAI-ENDPOINT\")\n","openai_api_version = get_secrets_from_kv(key_vault_name,\"AZURE-OPENAI-API-VERSION\")\n","\n","# Set up your Azure Text Analytics service and credentials  \n","COG_SERVICES_NAME = get_secrets_from_kv(key_vault_name,\"COG-SERVICES-NAME\")\n","COG_SERVICES_ENDPOINT = get_secrets_from_kv(key_vault_name,\"COG-SERVICES-ENDPOINT\")\n","COG_SERVICES_KEY = get_secrets_from_kv(key_vault_name,\"COG-SERVICES-KEY\")\n","\n","cog_services_credential = AzureKeyCredential(COG_SERVICES_KEY)  \n","\n","# Create a TextAnalyticsClient using your endpoint and credentials  \n","cog_services_client = TextAnalyticsClient(endpoint=COG_SERVICES_ENDPOINT, credential=cog_services_credential)  \n","\n","def get_named_entities(cog_services_client,input_text): \n","    # Call the named entity recognition API to extract named entities from your text  \n","    result = cog_services_client.recognize_entities(documents=[input_text])  \n","    \n","    # return the named entities for each document \n","    # full list of categories #https://learn.microsoft.com/en-us/azure/ai-services/language-service/named-entity-recognition/concepts/named-entity-categories?tabs=ga-api \n","\n","    Person = [] \n","    Location = []\n","    Organization = [] \n","    DateTime = []\n","    URL = [] \n","    Email = []\n","    PersonType = []\n","    Event = []\n","    Quantity = []\n","\n","    for idx, doc in enumerate(result):\n","        if not doc.is_error:\n","            for entity in doc.entities: \n","                if entity.category == \"DateTime\":\n","                    DateTime.append(entity.text)\n","                elif entity.category == \"Person\":\n","                    Person.append(entity.text)\n","                elif entity.category == \"Location\":\n","                    Location.append(entity.text)\n","                elif entity.category == \"Organization\":\n","                    Organization.append(entity.text)\n","                elif entity.category == \"URL\":\n","                    URL.append(entity.text)\n","                elif entity.category == \"Email\":\n","                    Email.append(entity.text)\n","                elif entity.category == \"PersonType\":\n","                    PersonType.append(entity.text)\n","                elif entity.category == \"Event\":\n","                    Event.append(entity.text)\n","                elif entity.category == \"Quantity\":\n","                    Quantity.append(entity.text)\n","\n","        else:  \n","            print(\"  Error: {}\".format(doc.error.message)) \n","    return(list(set(DateTime)),list(set(Person)),list(set(Location)),list(set(Organization)),list(set(URL)),list(set(Email)),list(set(PersonType)),list(set(Event)),list(set(Quantity)))\n","    \n","\n","from openai import AzureOpenAI\n","\n","# Function: Get Embeddings\n","def get_embeddings(text: str,openai_api_base,openai_api_version,openai_api_key):\n","    model_id = \"text-embedding-ada-002\"\n","    client = AzureOpenAI(\n","        api_version=openai_api_version,\n","        azure_endpoint=openai_api_base,\n","        api_key = openai_api_key\n","    )\n","    \n","    embedding = client.embeddings.create(input=text, model=model_id).data[0].embedding\n","\n","    return embedding\n","\n","# from langchain.text_splitter import MarkdownTextSplitter, RecursiveCharacterTextSplitter, PythonCodeTextSplitter\n","# import tiktoken\n","\n","import re\n","\n","def clean_spaces_with_regex(text):\n","    # Use a regular expression to replace multiple spaces with a single space\n","    cleaned_text = re.sub(r'\\s+', ' ', text)\n","    # Use a regular expression to replace consecutive dots with a single dot\n","    cleaned_text = re.sub(r'\\.{2,}', '.', cleaned_text)\n","    return cleaned_text\n","\n","def chunk_data(text):\n","    tokens_per_chunk = 500 #1024\n","    text = clean_spaces_with_regex(text)\n","    SENTENCE_ENDINGS = [\".\", \"!\", \"?\"]\n","    WORDS_BREAKS = ['\\n', '\\t', '}', '{', ']', '[', ')', '(', ' ', ':', ';', ',']\n","\n","    sentences = text.split('. ') # Split text into sentences\n","    chunks = []\n","    current_chunk = ''\n","    current_chunk_token_count = 0\n","    \n","    # Iterate through each sentence\n","    for sentence in sentences:\n","        # Split sentence into tokens\n","        tokens = sentence.split()\n","        \n","        # Check if adding the current sentence exceeds tokens_per_chunk\n","        if current_chunk_token_count + len(tokens) <= tokens_per_chunk:\n","            # Add the sentence to the current chunk\n","            if current_chunk:\n","                current_chunk += '. ' + sentence\n","            else:\n","                current_chunk += sentence\n","            current_chunk_token_count += len(tokens)\n","        else:\n","            # Add current chunk to chunks list and start a new chunk\n","            chunks.append(current_chunk)\n","            current_chunk = sentence\n","            current_chunk_token_count = len(tokens)\n","    \n","    # Add the last chunk\n","    if current_chunk:\n","        chunks.append(current_chunk)\n","    \n","    return chunks\n","\n","# def estimate_tokens(text):\n","#     GPT2_TOKENIZER = tiktoken.get_encoding(\"gpt2\")\n","#     return(len(GPT2_TOKENIZER.encode(text)))\n","\n","# def chunk_data(text):\n","#     text = clean_spaces_with_regex(text)\n","#     SENTENCE_ENDINGS = [\".\", \"!\", \"?\"]\n","#     WORDS_BREAKS = ['\\n', '\\t', '}', '{', ']', '[', ')', '(', ' ', ':', ';', ',']\n","#     num_tokens = 1024 #500\n","#     min_chunk_size = 10\n","#     token_overlap = 0\n","\n","#     splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(separators=SENTENCE_ENDINGS + WORDS_BREAKS,chunk_size=num_tokens, chunk_overlap=token_overlap)\n","\n","#     return(splitter.split_text(text))"]},{"cell_type":"code","execution_count":null,"id":"900907a3-5498-4779-b3cc-40cb00dc86ca","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Create the search index\n","search_credential = AzureKeyCredential(search_key)\n","\n","index_client = SearchIndexClient(\n","    endpoint=search_endpoint, credential=search_credential)\n","\n","fields = [\n","    SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True, sortable=True, filterable=True, facetable=True),\n","    SearchableField(name=\"chunk_id\", type=SearchFieldDataType.String),\n","    SearchableField(name=\"document_id\", type=SearchFieldDataType.String),\n","    SearchableField(name=\"title\", type=SearchFieldDataType.String),\n","    SearchableField(name=\"content\", type=SearchFieldDataType.String),\n","    SearchableField(name=\"sourceurl\", type=SearchFieldDataType.String),\n","    SearchableField(name=\"publicurl\", type=SearchFieldDataType.String),\n","    SimpleField(name=\"dateTime\", type=SearchFieldDataType.Collection(SearchFieldDataType.String),Filterable=True,Sortable=True, Facetable=True),\n","    SimpleField(name=\"Person\", type=SearchFieldDataType.Collection(SearchFieldDataType.String),Filterable=True,Sortable=True, Facetable=True),\n","    SimpleField(name=\"Location\", type=SearchFieldDataType.Collection(SearchFieldDataType.String),Filterable=True,Sortable=True, Facetable=True),\n","    SimpleField(name=\"Organization\", type=SearchFieldDataType.Collection(SearchFieldDataType.String),Filterable=True,Sortable=True, Facetable=True),\n","    SimpleField(name=\"URL\", type=SearchFieldDataType.Collection(SearchFieldDataType.String),Filterable=True,Sortable=True, Facetable=True),\n","    SimpleField(name=\"Email\", type=SearchFieldDataType.Collection(SearchFieldDataType.String),Filterable=True,Sortable=True, Facetable=True),\n","    SimpleField(name=\"PersonType\", type=SearchFieldDataType.Collection(SearchFieldDataType.String),Filterable=True,Sortable=True, Facetable=True),\n","    SimpleField(name=\"Event\", type=SearchFieldDataType.Collection(SearchFieldDataType.String),Filterable=True,Sortable=True, Facetable=True),\n","    SimpleField(name=\"Quantity\", type=SearchFieldDataType.Collection(SearchFieldDataType.String),Filterable=True,Sortable=True, Facetable=True),\n","    SearchField(name=\"titleVector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n","                searchable=True, vector_search_dimensions=1536, vector_search_profile_name=\"myHnswProfile\"),\n","    SearchField(name=\"contentVector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n","                searchable=True, vector_search_dimensions=1536, vector_search_profile_name=\"myHnswProfile\")\n","]\n","\n","# Configure the vector search configuration  \n","vector_search = VectorSearch(\n","    algorithms=[\n","        HnswAlgorithmConfiguration(\n","            name=\"myHnsw\",\n","            kind=VectorSearchAlgorithmKind.HNSW,\n","            parameters=HnswParameters(\n","                m=4,\n","                ef_construction=400,\n","                ef_search=500,\n","                metric=VectorSearchAlgorithmMetric.COSINE\n","            )\n","        ),\n","        ExhaustiveKnnAlgorithmConfiguration(\n","            name=\"myExhaustiveKnn\",\n","            kind=VectorSearchAlgorithmKind.EXHAUSTIVE_KNN,\n","            parameters=ExhaustiveKnnParameters(\n","                metric=VectorSearchAlgorithmMetric.COSINE\n","            )\n","        )\n","    ],\n","    profiles=[\n","        VectorSearchProfile(\n","            name=\"myHnswProfile\",\n","            algorithm_configuration_name=\"myHnsw\",\n","        ),\n","        VectorSearchProfile(\n","            name=\"myExhaustiveKnnProfile\",\n","            algorithm_configuration_name=\"myExhaustiveKnn\",\n","        )\n","    ]\n",")\n","\n","semantic_config = SemanticConfiguration(\n","    name=\"my-semantic-config\",\n","    prioritized_fields=SemanticPrioritizedFields(\n","        title_field=SemanticField(field_name=\"title\"),\n","        content_fields=[SemanticField(field_name=\"content\")]\n","    )\n",")\n","\n","# Create the semantic settings with the configuration\n","semantic_search = SemanticSearch(configurations=[semantic_config])\n","\n","# Create the drafts search index with the semantic settings\n","index = SearchIndex(name=index_name, fields=fields,\n","                    vector_search=vector_search, semantic_search=semantic_search)\n","\n","result = index_client.create_or_update_index(index)\n","print(f' {result.name} created')"]},{"cell_type":"code","execution_count":null,"id":"255e0a09-150e-495c-a412-377fea573760","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["#add documents to the index\n","\n","import pypdf  \n","import base64\n","import time\n","import pandas as pd\n","\n","account_name = get_secrets_from_kv(key_vault_name, \"ADLS-ACCOUNT-NAME\")\n","path_name = 'Files/' + account_name + '/' + directory + '/pdfs'\n","paths = mssparkutils.fs.ls(path_name)\n","\n","search_credential = AzureKeyCredential(search_key)\n","client = SearchClient(search_endpoint, index_name, search_credential)\n","drafts_client = SearchClient(search_endpoint, drafts_index_name, search_credential)\n","index_client = SearchIndexClient(endpoint=search_endpoint, credential=search_credential)\n","\n","metadata_filepath = 'Files/' + account_name + '/' + directory + '/metadata/' + csv_file_name\n","df_metadata = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"multiLine\", \"true\").option(\"quote\", \"\\\"\").option(\"escape\", \"\\\"\").load(metadata_filepath).toPandas()\n","\n","docs = []\n","num_pdfs = 0\n","counter = 0\n","for path in paths:\n","    num_pdfs += 1\n","    pdf_file_path = '/lakehouse/default/Files/' + account_name + '/' + directory + '/pdfs/' + path.name\n","    pdf_reader = PyPDF2.PdfReader(pdf_file_path)\n","    filename = path.name.split('/')[-1]\n","    document_id = filename.replace('.pdf','')\n","\n","    df_file_metadata = df_metadata[df_metadata['grant_id']==document_id].iloc[0]\n","   \n","    text = \"\" \n","\n","    n = num_pages #len(pdf_reader.pages)\n","    if len(pdf_reader.pages) < n:\n","        n = len(pdf_reader.pages)\n","    for page_num in range(n):\n","\n","        public_url = df_file_metadata['publicurl'] + '#page=' + str(page_num) \n","        page = pdf_reader.pages[page_num]\n","        text = page.extract_text()         \n","        \n","        chunks = chunk_data(text)\n","        chunk_num = 0\n","        for chunk in chunks:\n","            chunk_num += 1\n","            d = {\n","                \"chunk_id\" : path.name.split('/')[-1] + '_' + str(page_num).zfill(2) +  '_' + str(chunk_num).zfill(2),\n","                \"document_id\": str(df_file_metadata['grant_id']),\n","                 \"content\": chunk,       \n","                 \"title\": df_file_metadata['title']\n","                }\n","\n","            d[\"dateTime\"],d[\"Person\"],d[\"Location\"],d[\"Organization\"],d[\"URL\"],d[\"Email\"],d[\"PersonType\"],d[\"Event\"],d[\"Quantity\"] = get_named_entities(cog_services_client,d[\"content\"])\n","\n","            counter += 1\n","\n","            try:\n","                v_titleVector = get_embeddings(d[\"title\"],openai_api_base,openai_api_version,openai_api_key)\n","            except:\n","                time.sleep(30)\n","                v_titleVector = get_embeddings(d[\"title\"],openai_api_base,openai_api_version,openai_api_key)\n","            \n","            try:\n","                v_contentVector = get_embeddings(d[\"content\"],openai_api_base,openai_api_version,openai_api_key)\n","            except:\n","                time.sleep(30)\n","                v_contentVector = get_embeddings(d[\"content\"],openai_api_base,openai_api_version,openai_api_key)\n","\n","\n","            docs.append(\n","            {\n","                    \"id\": base64.urlsafe_b64encode(bytes(d[\"chunk_id\"], encoding='utf-8')).decode('utf-8'),\n","                    \"chunk_id\": d[\"chunk_id\"],\n","                    \"document_id\": d[\"document_id\"],\n","                    \"title\": d[\"title\"],\n","                    \"content\": d[\"content\"],\n","                    \"sourceurl\": path.name.split('/')[-1], \n","                    \"publicurl\": public_url,\n","                    \"dateTime\": d[\"dateTime\"],\n","                    \"Person\": d[\"Person\"],\n","                    \"Location\": d[\"Location\"],\n","                    \"Organization\": d[\"Organization\"],\n","                    \"URL\": d[\"URL\"],\n","                    \"Email\": d[\"Email\"],\n","                    \"PersonType\": d[\"PersonType\"],\n","                    \"Event\": d[\"Event\"],\n","                    \"Quantity\": d[\"Quantity\"],\n","                    \"titleVector\": v_titleVector,\n","                    \"contentVector\": v_contentVector\n","            }\n","            )\n","            \n","            if counter % 10 == 0:\n","                result = client.upload_documents(documents=docs)\n","                result = drafts_client.upload_documents(documents=docs)\n","                docs = []\n","                print(f' {str(counter)} uploaded')\n","                \n","#upload the last batch\n","if docs != []:\n","    client.upload_documents(documents=docs)\n","    drafts_client.upload_documents(documents=docs)"]}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","language":"Python","name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python"},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default"},"synapse_widget":{"state":{},"version":"0.1"},"trident":{"lakehouse":{"default_lakehouse":"12c0394e-b329-4040-9f9f-6bcc4e09823b","default_lakehouse_name":"BYCLakehouse","default_lakehouse_workspace_id":"f5fe8dc7-ae07-4a98-af91-8933cd2e381c"}},"widgets":{}},"nbformat":4,"nbformat_minor":5}
